{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a67c1551-92d6-4590-96cf-3f63f88666b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import nltk\n",
    "import eli5\n",
    "import utils\n",
    "from sklearn_crfsuite import CRF\n",
    "from IPython.display import display, HTML\n",
    "import os\n",
    "import gensim.models.keyedvectors as word2vec\n",
    "from nltk.corpus import brown,reuters , gutenberg\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn_crfsuite import CRF\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb541631-de50-45b4-aa08-bb43ddfcbde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_FILE = \"./data/tagged_sentences_examples\"\n",
    "\n",
    "examples = list(pd.read_csv(\"./data/idiom_example.csv\")[\"sentence\"])\n",
    "\n",
    "\n",
    "\n",
    "sents = [word_tokenize(sent.replace(\"’\", \"'\").replace(\"–\", \"-\")) \\\n",
    "         for sent in examples]\n",
    "\n",
    "#utils.clear_file(OUTPUT_FILE)\n",
    "\n",
    "#success = utils.write_tagged_sentences(sents, OUTPUT_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21419306-0690-4ce6-9245-f06a9b6cf10b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package reuters to /home/yasser/nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n",
      "[nltk_data] Downloading package brown to /home/yasser/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package gutenberg to /home/yasser/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/yasser/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('reuters')\n",
    "nltk.download('brown')\n",
    "nltk.download('gutenberg')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f20b8e4-985e-4148-a850-61b8facf72e6",
   "metadata": {},
   "source": [
    "## Building the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8fff410-33fc-41ba-b419-f7bc0db457b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting 2script execution: 2022-12-30 18:55:19.404466\n",
      "2022-12-30 18:55:21.274885: Tagged 0 of 57340 sentences...\n",
      "2022-12-30 18:55:22.119219: Tagged 500 of 57340 sentences...\n",
      "2022-12-30 18:55:23.020404: Tagged 1000 of 57340 sentences...\n",
      "2022-12-30 18:55:23.870239: Tagged 1500 of 57340 sentences...\n",
      "2022-12-30 18:55:24.843260: Tagged 2000 of 57340 sentences...\n",
      "2022-12-30 18:55:25.627429: Tagged 2500 of 57340 sentences...\n",
      "2022-12-30 18:55:26.444585: Tagged 3000 of 57340 sentences...\n",
      "2022-12-30 18:55:27.316249: Tagged 3500 of 57340 sentences...\n",
      "2022-12-30 18:55:28.183657: Tagged 4000 of 57340 sentences...\n",
      "2022-12-30 18:55:29.112707: Tagged 4500 of 57340 sentences...\n",
      "2022-12-30 18:55:29.954928: Tagged 5000 of 57340 sentences...\n",
      "2022-12-30 18:55:30.792915: Tagged 5500 of 57340 sentences...\n",
      "2022-12-30 18:55:31.514850: Tagged 6000 of 57340 sentences...\n",
      "2022-12-30 18:55:32.304588: Tagged 6500 of 57340 sentences...\n",
      "2022-12-30 18:55:33.128942: Tagged 7000 of 57340 sentences...\n",
      "2022-12-30 18:55:33.947276: Tagged 7500 of 57340 sentences...\n",
      "2022-12-30 18:55:34.774221: Tagged 8000 of 57340 sentences...\n",
      "2022-12-30 18:55:35.530841: Tagged 8500 of 57340 sentences...\n",
      "2022-12-30 18:55:36.387447: Tagged 9000 of 57340 sentences...\n",
      "2022-12-30 18:55:37.323242: Tagged 9500 of 57340 sentences...\n",
      "2022-12-30 18:55:38.239032: Tagged 10000 of 57340 sentences...\n",
      "2022-12-30 18:55:39.099980: Tagged 10500 of 57340 sentences...\n",
      "2022-12-30 18:55:40.172806: Tagged 11000 of 57340 sentences...\n",
      "2022-12-30 18:55:41.102637: Tagged 11500 of 57340 sentences...\n",
      "2022-12-30 18:55:42.467020: Tagged 12000 of 57340 sentences...\n",
      "2022-12-30 18:55:43.890817: Tagged 12500 of 57340 sentences...\n",
      "2022-12-30 18:55:44.828282: Tagged 13000 of 57340 sentences...\n",
      "2022-12-30 18:55:46.233304: Tagged 13500 of 57340 sentences...\n",
      "2022-12-30 18:55:47.538808: Tagged 14000 of 57340 sentences...\n",
      "2022-12-30 18:55:48.774971: Tagged 14500 of 57340 sentences...\n",
      "2022-12-30 18:55:49.654714: Tagged 15000 of 57340 sentences...\n",
      "2022-12-30 18:55:50.545606: Tagged 15500 of 57340 sentences...\n",
      "2022-12-30 18:55:51.501360: Tagged 16000 of 57340 sentences...\n",
      "2022-12-30 18:55:52.402825: Tagged 16500 of 57340 sentences...\n",
      "2022-12-30 18:55:53.230211: Tagged 17000 of 57340 sentences...\n",
      "2022-12-30 18:55:54.081360: Tagged 17500 of 57340 sentences...\n",
      "2022-12-30 18:55:54.870093: Tagged 18000 of 57340 sentences...\n",
      "2022-12-30 18:55:55.646413: Tagged 18500 of 57340 sentences...\n",
      "2022-12-30 18:55:56.446232: Tagged 19000 of 57340 sentences...\n",
      "2022-12-30 18:55:57.342215: Tagged 19500 of 57340 sentences...\n",
      "2022-12-30 18:55:58.210953: Tagged 20000 of 57340 sentences...\n",
      "2022-12-30 18:55:59.086974: Tagged 20500 of 57340 sentences...\n",
      "2022-12-30 18:56:00.044516: Tagged 21000 of 57340 sentences...\n",
      "2022-12-30 18:56:00.816032: Tagged 21500 of 57340 sentences...\n",
      "2022-12-30 18:56:01.592863: Tagged 22000 of 57340 sentences...\n",
      "2022-12-30 18:56:02.420588: Tagged 22500 of 57340 sentences...\n",
      "2022-12-30 18:56:03.273399: Tagged 23000 of 57340 sentences...\n",
      "2022-12-30 18:56:04.105587: Tagged 23500 of 57340 sentences...\n",
      "2022-12-30 18:56:04.926061: Tagged 24000 of 57340 sentences...\n",
      "2022-12-30 18:56:05.677205: Tagged 24500 of 57340 sentences...\n",
      "2022-12-30 18:56:06.500699: Tagged 25000 of 57340 sentences...\n",
      "2022-12-30 18:56:07.308864: Tagged 25500 of 57340 sentences...\n",
      "2022-12-30 18:56:08.146877: Tagged 26000 of 57340 sentences...\n",
      "2022-12-30 18:56:09.018188: Tagged 26500 of 57340 sentences...\n",
      "2022-12-30 18:56:09.898841: Tagged 27000 of 57340 sentences...\n",
      "2022-12-30 18:56:10.634141: Tagged 27500 of 57340 sentences...\n",
      "2022-12-30 18:56:11.511040: Tagged 28000 of 57340 sentences...\n",
      "2022-12-30 18:56:12.359169: Tagged 28500 of 57340 sentences...\n",
      "2022-12-30 18:56:13.162081: Tagged 29000 of 57340 sentences...\n",
      "2022-12-30 18:56:13.993844: Tagged 29500 of 57340 sentences...\n",
      "2022-12-30 18:56:14.783123: Tagged 30000 of 57340 sentences...\n",
      "2022-12-30 18:56:15.648005: Tagged 30500 of 57340 sentences...\n",
      "2022-12-30 18:56:16.460644: Tagged 31000 of 57340 sentences...\n",
      "2022-12-30 18:56:17.261801: Tagged 31500 of 57340 sentences...\n",
      "2022-12-30 18:56:18.100710: Tagged 32000 of 57340 sentences...\n",
      "2022-12-30 18:56:18.905727: Tagged 32500 of 57340 sentences...\n",
      "2022-12-30 18:56:19.687500: Tagged 33000 of 57340 sentences...\n",
      "2022-12-30 18:56:20.513022: Tagged 33500 of 57340 sentences...\n",
      "2022-12-30 18:56:21.353235: Tagged 34000 of 57340 sentences...\n",
      "2022-12-30 18:56:22.190950: Tagged 34500 of 57340 sentences...\n",
      "2022-12-30 18:56:22.997323: Tagged 35000 of 57340 sentences...\n",
      "2022-12-30 18:56:23.767832: Tagged 35500 of 57340 sentences...\n",
      "2022-12-30 18:56:24.620446: Tagged 36000 of 57340 sentences...\n",
      "2022-12-30 18:56:25.454835: Tagged 36500 of 57340 sentences...\n",
      "2022-12-30 18:56:26.286046: Tagged 37000 of 57340 sentences...\n",
      "2022-12-30 18:56:27.092425: Tagged 37500 of 57340 sentences...\n",
      "2022-12-30 18:56:27.831557: Tagged 38000 of 57340 sentences...\n",
      "2022-12-30 18:56:28.595263: Tagged 38500 of 57340 sentences...\n",
      "2022-12-30 18:56:29.352992: Tagged 39000 of 57340 sentences...\n",
      "2022-12-30 18:56:30.138443: Tagged 39500 of 57340 sentences...\n",
      "2022-12-30 18:56:30.829210: Tagged 40000 of 57340 sentences...\n",
      "2022-12-30 18:56:31.512004: Tagged 40500 of 57340 sentences...\n",
      "2022-12-30 18:56:32.254920: Tagged 41000 of 57340 sentences...\n",
      "2022-12-30 18:56:32.991617: Tagged 41500 of 57340 sentences...\n",
      "2022-12-30 18:56:33.745172: Tagged 42000 of 57340 sentences...\n",
      "2022-12-30 18:56:34.436031: Tagged 42500 of 57340 sentences...\n",
      "2022-12-30 18:56:35.259553: Tagged 43000 of 57340 sentences...\n",
      "2022-12-30 18:56:35.954115: Tagged 43500 of 57340 sentences...\n",
      "2022-12-30 18:56:36.638182: Tagged 44000 of 57340 sentences...\n",
      "2022-12-30 18:56:37.404913: Tagged 44500 of 57340 sentences...\n",
      "2022-12-30 18:56:38.056887: Tagged 45000 of 57340 sentences...\n",
      "2022-12-30 18:56:38.759331: Tagged 45500 of 57340 sentences...\n",
      "2022-12-30 18:56:39.545317: Tagged 46000 of 57340 sentences...\n",
      "2022-12-30 18:56:40.209385: Tagged 46500 of 57340 sentences...\n",
      "2022-12-30 18:56:40.876446: Tagged 47000 of 57340 sentences...\n",
      "2022-12-30 18:56:41.656688: Tagged 47500 of 57340 sentences...\n",
      "2022-12-30 18:56:42.295563: Tagged 48000 of 57340 sentences...\n",
      "2022-12-30 18:56:43.006227: Tagged 48500 of 57340 sentences...\n",
      "2022-12-30 18:56:43.704702: Tagged 49000 of 57340 sentences...\n",
      "2022-12-30 18:56:44.383500: Tagged 49500 of 57340 sentences...\n",
      "2022-12-30 18:56:45.156593: Tagged 50000 of 57340 sentences...\n",
      "2022-12-30 18:56:45.913458: Tagged 50500 of 57340 sentences...\n",
      "2022-12-30 18:56:46.663721: Tagged 51000 of 57340 sentences...\n",
      "2022-12-30 18:56:47.431156: Tagged 51500 of 57340 sentences...\n",
      "2022-12-30 18:56:48.105920: Tagged 52000 of 57340 sentences...\n",
      "2022-12-30 18:56:48.819377: Tagged 52500 of 57340 sentences...\n",
      "2022-12-30 18:56:49.575937: Tagged 53000 of 57340 sentences...\n",
      "2022-12-30 18:56:50.294751: Tagged 53500 of 57340 sentences...\n",
      "2022-12-30 18:56:50.954146: Tagged 54000 of 57340 sentences...\n",
      "2022-12-30 18:56:51.739808: Tagged 54500 of 57340 sentences...\n",
      "2022-12-30 18:56:52.384735: Tagged 55000 of 57340 sentences...\n",
      "2022-12-30 18:56:53.100727: Tagged 55500 of 57340 sentences...\n",
      "2022-12-30 18:56:53.812036: Tagged 56000 of 57340 sentences...\n",
      "2022-12-30 18:56:54.507630: Tagged 56500 of 57340 sentences...\n",
      "2022-12-30 18:56:55.466021: Tagged 57000 of 57340 sentences...\n",
      "Finishing script execution: 2022-12-30 18:56:55.962427\n",
      "number of idioms found (ish): 431\n"
     ]
    }
   ],
   "source": [
    "# Brown\n",
    "OUTPUT_FILE = \"./data/tagged_sentences_brown\"\n",
    "\n",
    "sents = brown.sents()\n",
    "\n",
    "# Clear contents if the file exists\n",
    "utils.clear_file(OUTPUT_FILE)\n",
    "\n",
    "\n",
    "## write to the file\n",
    "success = utils.write_tagged_sentences(sents, OUTPUT_FILE)\n",
    "\n",
    "if success:\n",
    "    count = utils.num_found_idioms(OUTPUT_FILE)\n",
    "    print(\"number of idioms found (ish): {}\".format(count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "baf9ee51-a641-4086-8e45-38e788a27f8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting 2script execution: 2022-12-30 18:59:38.879827\n",
      "2022-12-30 18:59:38.879909: Tagged 0 of 238 sentences...\n",
      "Finishing script execution: 2022-12-30 18:59:39.240309\n",
      "number of idioms found (ish): 149\n"
     ]
    }
   ],
   "source": [
    "OUTPUT_FILE = \"./data/tagged_sentences_examples\"\n",
    "\n",
    "examples = list(pd.read_csv(\"./data/idiom_example.csv\")[\"sentence\"])\n",
    "\n",
    "\n",
    "\n",
    "sents = [word_tokenize(sent.replace(\"’\", \"'\").replace(\"–\", \"-\")) \\\n",
    "         for sent in examples]\n",
    "\n",
    "utils.clear_file(OUTPUT_FILE)\n",
    "\n",
    "success = utils.write_tagged_sentences(sents, OUTPUT_FILE)\n",
    "\n",
    "if success:\n",
    "    count = utils.num_found_idioms(OUTPUT_FILE)\n",
    "    print(\"number of idioms found (ish): {}\".format(count))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ec063a5-ecbf-44a8-8f85-dafaa8733849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting 2script execution: 2022-12-30 18:59:47.282680\n",
      "2022-12-30 18:59:51.475500: Tagged 0 of 98552 sentences...\n",
      "2022-12-30 18:59:52.333763: Tagged 500 of 98552 sentences...\n",
      "2022-12-30 18:59:53.131030: Tagged 1000 of 98552 sentences...\n",
      "2022-12-30 18:59:53.907867: Tagged 1500 of 98552 sentences...\n",
      "2022-12-30 18:59:54.680018: Tagged 2000 of 98552 sentences...\n",
      "2022-12-30 18:59:55.530692: Tagged 2500 of 98552 sentences...\n",
      "2022-12-30 18:59:56.346159: Tagged 3000 of 98552 sentences...\n",
      "2022-12-30 18:59:57.145586: Tagged 3500 of 98552 sentences...\n",
      "2022-12-30 18:59:57.933738: Tagged 4000 of 98552 sentences...\n",
      "2022-12-30 18:59:58.648395: Tagged 4500 of 98552 sentences...\n",
      "2022-12-30 18:59:59.440211: Tagged 5000 of 98552 sentences...\n",
      "2022-12-30 19:00:00.228681: Tagged 5500 of 98552 sentences...\n",
      "2022-12-30 19:00:01.045270: Tagged 6000 of 98552 sentences...\n",
      "2022-12-30 19:00:01.807035: Tagged 6500 of 98552 sentences...\n",
      "2022-12-30 19:00:02.584245: Tagged 7000 of 98552 sentences...\n",
      "2022-12-30 19:00:03.413807: Tagged 7500 of 98552 sentences...\n",
      "2022-12-30 19:00:04.264108: Tagged 8000 of 98552 sentences...\n",
      "2022-12-30 19:00:05.100597: Tagged 8500 of 98552 sentences...\n",
      "2022-12-30 19:00:05.920298: Tagged 9000 of 98552 sentences...\n",
      "2022-12-30 19:00:06.663020: Tagged 9500 of 98552 sentences...\n",
      "2022-12-30 19:00:07.473274: Tagged 10000 of 98552 sentences...\n",
      "2022-12-30 19:00:08.251109: Tagged 10500 of 98552 sentences...\n",
      "2022-12-30 19:00:09.033688: Tagged 11000 of 98552 sentences...\n",
      "2022-12-30 19:00:09.758688: Tagged 11500 of 98552 sentences...\n",
      "2022-12-30 19:00:10.593458: Tagged 12000 of 98552 sentences...\n",
      "2022-12-30 19:00:11.394093: Tagged 12500 of 98552 sentences...\n",
      "2022-12-30 19:00:12.178829: Tagged 13000 of 98552 sentences...\n",
      "2022-12-30 19:00:13.022944: Tagged 13500 of 98552 sentences...\n",
      "2022-12-30 19:00:13.866270: Tagged 14000 of 98552 sentences...\n",
      "2022-12-30 19:00:14.620504: Tagged 14500 of 98552 sentences...\n",
      "2022-12-30 19:00:15.472439: Tagged 15000 of 98552 sentences...\n",
      "2022-12-30 19:00:16.309348: Tagged 15500 of 98552 sentences...\n",
      "2022-12-30 19:00:17.159845: Tagged 16000 of 98552 sentences...\n",
      "2022-12-30 19:00:18.042916: Tagged 16500 of 98552 sentences...\n",
      "2022-12-30 19:00:18.842509: Tagged 17000 of 98552 sentences...\n",
      "2022-12-30 19:00:19.694893: Tagged 17500 of 98552 sentences...\n",
      "2022-12-30 19:00:20.528673: Tagged 18000 of 98552 sentences...\n",
      "2022-12-30 19:00:21.383515: Tagged 18500 of 98552 sentences...\n",
      "2022-12-30 19:00:22.289171: Tagged 19000 of 98552 sentences...\n",
      "2022-12-30 19:00:23.209954: Tagged 19500 of 98552 sentences...\n",
      "2022-12-30 19:00:24.159559: Tagged 20000 of 98552 sentences...\n",
      "2022-12-30 19:00:25.061516: Tagged 20500 of 98552 sentences...\n",
      "2022-12-30 19:00:26.016610: Tagged 21000 of 98552 sentences...\n",
      "2022-12-30 19:00:26.958424: Tagged 21500 of 98552 sentences...\n",
      "2022-12-30 19:00:27.838887: Tagged 22000 of 98552 sentences...\n",
      "2022-12-30 19:00:28.701568: Tagged 22500 of 98552 sentences...\n",
      "2022-12-30 19:00:29.577991: Tagged 23000 of 98552 sentences...\n",
      "2022-12-30 19:00:30.427294: Tagged 23500 of 98552 sentences...\n",
      "2022-12-30 19:00:31.290641: Tagged 24000 of 98552 sentences...\n",
      "2022-12-30 19:00:32.136634: Tagged 24500 of 98552 sentences...\n",
      "2022-12-30 19:00:33.024601: Tagged 25000 of 98552 sentences...\n",
      "2022-12-30 19:00:33.878923: Tagged 25500 of 98552 sentences...\n",
      "2022-12-30 19:00:34.805536: Tagged 26000 of 98552 sentences...\n",
      "2022-12-30 19:00:35.626047: Tagged 26500 of 98552 sentences...\n",
      "2022-12-30 19:00:36.509579: Tagged 27000 of 98552 sentences...\n",
      "2022-12-30 19:00:37.477680: Tagged 27500 of 98552 sentences...\n",
      "2022-12-30 19:00:38.303833: Tagged 28000 of 98552 sentences...\n",
      "2022-12-30 19:00:39.306376: Tagged 28500 of 98552 sentences...\n",
      "2022-12-30 19:00:40.077774: Tagged 29000 of 98552 sentences...\n",
      "2022-12-30 19:00:40.796137: Tagged 29500 of 98552 sentences...\n",
      "2022-12-30 19:00:41.648639: Tagged 30000 of 98552 sentences...\n",
      "2022-12-30 19:00:42.379748: Tagged 30500 of 98552 sentences...\n",
      "2022-12-30 19:00:43.218674: Tagged 31000 of 98552 sentences...\n",
      "2022-12-30 19:00:43.932841: Tagged 31500 of 98552 sentences...\n",
      "2022-12-30 19:00:44.640077: Tagged 32000 of 98552 sentences...\n",
      "2022-12-30 19:00:45.463458: Tagged 32500 of 98552 sentences...\n",
      "2022-12-30 19:00:46.196966: Tagged 33000 of 98552 sentences...\n",
      "2022-12-30 19:00:46.965683: Tagged 33500 of 98552 sentences...\n",
      "2022-12-30 19:00:47.822864: Tagged 34000 of 98552 sentences...\n",
      "2022-12-30 19:00:48.595535: Tagged 34500 of 98552 sentences...\n",
      "2022-12-30 19:00:49.513018: Tagged 35000 of 98552 sentences...\n",
      "2022-12-30 19:00:50.294297: Tagged 35500 of 98552 sentences...\n",
      "2022-12-30 19:00:51.266266: Tagged 36000 of 98552 sentences...\n",
      "2022-12-30 19:00:52.054341: Tagged 36500 of 98552 sentences...\n",
      "2022-12-30 19:00:53.055918: Tagged 37000 of 98552 sentences...\n",
      "2022-12-30 19:00:54.008001: Tagged 37500 of 98552 sentences...\n",
      "2022-12-30 19:00:54.843546: Tagged 38000 of 98552 sentences...\n",
      "2022-12-30 19:00:55.715801: Tagged 38500 of 98552 sentences...\n",
      "2022-12-30 19:00:56.507924: Tagged 39000 of 98552 sentences...\n",
      "2022-12-30 19:00:57.373301: Tagged 39500 of 98552 sentences...\n",
      "2022-12-30 19:00:58.122283: Tagged 40000 of 98552 sentences...\n",
      "2022-12-30 19:00:58.872438: Tagged 40500 of 98552 sentences...\n",
      "2022-12-30 19:00:59.738803: Tagged 41000 of 98552 sentences...\n",
      "2022-12-30 19:01:00.495789: Tagged 41500 of 98552 sentences...\n",
      "2022-12-30 19:01:01.356377: Tagged 42000 of 98552 sentences...\n",
      "2022-12-30 19:01:02.085389: Tagged 42500 of 98552 sentences...\n",
      "2022-12-30 19:01:02.845394: Tagged 43000 of 98552 sentences...\n",
      "2022-12-30 19:01:03.734245: Tagged 43500 of 98552 sentences...\n",
      "2022-12-30 19:01:04.533889: Tagged 44000 of 98552 sentences...\n",
      "2022-12-30 19:01:05.389179: Tagged 44500 of 98552 sentences...\n",
      "2022-12-30 19:01:06.154344: Tagged 45000 of 98552 sentences...\n",
      "2022-12-30 19:01:06.961401: Tagged 45500 of 98552 sentences...\n",
      "2022-12-30 19:01:07.858116: Tagged 46000 of 98552 sentences...\n",
      "2022-12-30 19:01:08.662620: Tagged 46500 of 98552 sentences...\n",
      "2022-12-30 19:01:09.499520: Tagged 47000 of 98552 sentences...\n",
      "2022-12-30 19:01:10.185761: Tagged 47500 of 98552 sentences...\n",
      "2022-12-30 19:01:10.871261: Tagged 48000 of 98552 sentences...\n",
      "2022-12-30 19:01:11.651459: Tagged 48500 of 98552 sentences...\n",
      "2022-12-30 19:01:12.451382: Tagged 49000 of 98552 sentences...\n",
      "2022-12-30 19:01:13.230069: Tagged 49500 of 98552 sentences...\n",
      "2022-12-30 19:01:13.946028: Tagged 50000 of 98552 sentences...\n",
      "2022-12-30 19:01:14.699296: Tagged 50500 of 98552 sentences...\n",
      "2022-12-30 19:01:15.470388: Tagged 51000 of 98552 sentences...\n",
      "2022-12-30 19:01:16.238445: Tagged 51500 of 98552 sentences...\n",
      "2022-12-30 19:01:16.937713: Tagged 52000 of 98552 sentences...\n",
      "2022-12-30 19:01:17.687789: Tagged 52500 of 98552 sentences...\n",
      "2022-12-30 19:01:18.470692: Tagged 53000 of 98552 sentences...\n",
      "2022-12-30 19:01:19.240313: Tagged 53500 of 98552 sentences...\n",
      "2022-12-30 19:01:19.950854: Tagged 54000 of 98552 sentences...\n",
      "2022-12-30 19:01:20.690664: Tagged 54500 of 98552 sentences...\n",
      "2022-12-30 19:01:21.494398: Tagged 55000 of 98552 sentences...\n",
      "2022-12-30 19:01:22.330424: Tagged 55500 of 98552 sentences...\n",
      "2022-12-30 19:01:23.031832: Tagged 56000 of 98552 sentences...\n",
      "2022-12-30 19:01:23.821654: Tagged 56500 of 98552 sentences...\n",
      "2022-12-30 19:01:24.617580: Tagged 57000 of 98552 sentences...\n",
      "2022-12-30 19:01:25.424958: Tagged 57500 of 98552 sentences...\n",
      "2022-12-30 19:01:26.211411: Tagged 58000 of 98552 sentences...\n",
      "2022-12-30 19:01:26.916780: Tagged 58500 of 98552 sentences...\n",
      "2022-12-30 19:01:27.756556: Tagged 59000 of 98552 sentences...\n",
      "2022-12-30 19:01:28.538078: Tagged 59500 of 98552 sentences...\n",
      "2022-12-30 19:01:29.346594: Tagged 60000 of 98552 sentences...\n",
      "2022-12-30 19:01:30.060119: Tagged 60500 of 98552 sentences...\n",
      "2022-12-30 19:01:30.858326: Tagged 61000 of 98552 sentences...\n",
      "2022-12-30 19:01:31.639189: Tagged 61500 of 98552 sentences...\n",
      "2022-12-30 19:01:32.394819: Tagged 62000 of 98552 sentences...\n",
      "2022-12-30 19:01:33.120124: Tagged 62500 of 98552 sentences...\n",
      "2022-12-30 19:01:33.889048: Tagged 63000 of 98552 sentences...\n",
      "2022-12-30 19:01:34.701876: Tagged 63500 of 98552 sentences...\n",
      "2022-12-30 19:01:35.489279: Tagged 64000 of 98552 sentences...\n",
      "2022-12-30 19:01:36.284004: Tagged 64500 of 98552 sentences...\n",
      "2022-12-30 19:01:37.033251: Tagged 65000 of 98552 sentences...\n",
      "2022-12-30 19:01:37.871640: Tagged 65500 of 98552 sentences...\n",
      "2022-12-30 19:01:38.680852: Tagged 66000 of 98552 sentences...\n",
      "2022-12-30 19:01:39.526506: Tagged 66500 of 98552 sentences...\n",
      "2022-12-30 19:01:40.329792: Tagged 67000 of 98552 sentences...\n",
      "2022-12-30 19:01:41.076276: Tagged 67500 of 98552 sentences...\n",
      "2022-12-30 19:01:41.859453: Tagged 68000 of 98552 sentences...\n",
      "2022-12-30 19:01:42.586506: Tagged 68500 of 98552 sentences...\n",
      "2022-12-30 19:01:43.280668: Tagged 69000 of 98552 sentences...\n",
      "2022-12-30 19:01:43.899791: Tagged 69500 of 98552 sentences...\n",
      "2022-12-30 19:01:44.655607: Tagged 70000 of 98552 sentences...\n",
      "2022-12-30 19:01:45.481481: Tagged 70500 of 98552 sentences...\n",
      "2022-12-30 19:01:46.179702: Tagged 71000 of 98552 sentences...\n",
      "2022-12-30 19:01:46.946042: Tagged 71500 of 98552 sentences...\n",
      "2022-12-30 19:01:47.689405: Tagged 72000 of 98552 sentences...\n",
      "2022-12-30 19:01:48.472219: Tagged 72500 of 98552 sentences...\n",
      "2022-12-30 19:01:49.135664: Tagged 73000 of 98552 sentences...\n",
      "2022-12-30 19:01:49.925865: Tagged 73500 of 98552 sentences...\n",
      "2022-12-30 19:01:50.722597: Tagged 74000 of 98552 sentences...\n",
      "2022-12-30 19:01:51.535927: Tagged 74500 of 98552 sentences...\n",
      "2022-12-30 19:01:52.323525: Tagged 75000 of 98552 sentences...\n",
      "2022-12-30 19:01:53.025716: Tagged 75500 of 98552 sentences...\n",
      "2022-12-30 19:01:53.826222: Tagged 76000 of 98552 sentences...\n",
      "2022-12-30 19:01:54.661232: Tagged 76500 of 98552 sentences...\n",
      "2022-12-30 19:01:55.462802: Tagged 77000 of 98552 sentences...\n",
      "2022-12-30 19:01:56.181293: Tagged 77500 of 98552 sentences...\n",
      "2022-12-30 19:01:56.969626: Tagged 78000 of 98552 sentences...\n",
      "2022-12-30 19:01:57.790784: Tagged 78500 of 98552 sentences...\n",
      "2022-12-30 19:01:58.554557: Tagged 79000 of 98552 sentences...\n",
      "2022-12-30 19:01:59.389450: Tagged 79500 of 98552 sentences...\n",
      "2022-12-30 19:02:00.179519: Tagged 80000 of 98552 sentences...\n",
      "2022-12-30 19:02:01.072415: Tagged 80500 of 98552 sentences...\n",
      "2022-12-30 19:02:01.888306: Tagged 81000 of 98552 sentences...\n",
      "2022-12-30 19:02:02.723705: Tagged 81500 of 98552 sentences...\n",
      "2022-12-30 19:02:03.617808: Tagged 82000 of 98552 sentences...\n",
      "2022-12-30 19:02:04.445164: Tagged 82500 of 98552 sentences...\n",
      "2022-12-30 19:02:05.165236: Tagged 83000 of 98552 sentences...\n",
      "2022-12-30 19:02:05.984888: Tagged 83500 of 98552 sentences...\n",
      "2022-12-30 19:02:06.783159: Tagged 84000 of 98552 sentences...\n",
      "2022-12-30 19:02:07.539582: Tagged 84500 of 98552 sentences...\n",
      "2022-12-30 19:02:08.277797: Tagged 85000 of 98552 sentences...\n",
      "2022-12-30 19:02:09.102673: Tagged 85500 of 98552 sentences...\n",
      "2022-12-30 19:02:10.111854: Tagged 86000 of 98552 sentences...\n",
      "2022-12-30 19:02:11.131831: Tagged 86500 of 98552 sentences...\n",
      "2022-12-30 19:02:12.059897: Tagged 87000 of 98552 sentences...\n",
      "2022-12-30 19:02:12.875812: Tagged 87500 of 98552 sentences...\n",
      "2022-12-30 19:02:13.599482: Tagged 88000 of 98552 sentences...\n",
      "2022-12-30 19:02:14.253333: Tagged 88500 of 98552 sentences...\n",
      "2022-12-30 19:02:14.929370: Tagged 89000 of 98552 sentences...\n",
      "2022-12-30 19:02:15.659936: Tagged 89500 of 98552 sentences...\n",
      "2022-12-30 19:02:16.307401: Tagged 90000 of 98552 sentences...\n",
      "2022-12-30 19:02:17.000181: Tagged 90500 of 98552 sentences...\n",
      "2022-12-30 19:02:17.701280: Tagged 91000 of 98552 sentences...\n",
      "2022-12-30 19:02:18.355738: Tagged 91500 of 98552 sentences...\n",
      "2022-12-30 19:02:19.042112: Tagged 92000 of 98552 sentences...\n",
      "2022-12-30 19:02:19.750415: Tagged 92500 of 98552 sentences...\n",
      "2022-12-30 19:02:20.468490: Tagged 93000 of 98552 sentences...\n",
      "2022-12-30 19:02:21.108512: Tagged 93500 of 98552 sentences...\n",
      "2022-12-30 19:02:21.823635: Tagged 94000 of 98552 sentences...\n",
      "2022-12-30 19:02:22.582969: Tagged 94500 of 98552 sentences...\n",
      "2022-12-30 19:02:23.453567: Tagged 95000 of 98552 sentences...\n",
      "2022-12-30 19:02:24.269716: Tagged 95500 of 98552 sentences...\n",
      "2022-12-30 19:02:25.198058: Tagged 96000 of 98552 sentences...\n",
      "2022-12-30 19:02:26.027801: Tagged 96500 of 98552 sentences...\n",
      "2022-12-30 19:02:26.897517: Tagged 97000 of 98552 sentences...\n",
      "2022-12-30 19:02:27.781540: Tagged 97500 of 98552 sentences...\n",
      "2022-12-30 19:02:28.630396: Tagged 98000 of 98552 sentences...\n",
      "2022-12-30 19:02:29.499842: Tagged 98500 of 98552 sentences...\n",
      "Finishing script execution: 2022-12-30 19:02:29.580478\n",
      "number of idioms found (ish): 717\n"
     ]
    }
   ],
   "source": [
    "# gutenberg\n",
    "OUTPUT_FILE = \"./data/tagged_sentences_gutenberg\"\n",
    "\n",
    "# Clear contents if the file exists\n",
    "utils.clear_file(OUTPUT_FILE)\n",
    "\n",
    "## write to the file\n",
    "success = utils.write_tagged_sentences(gutenberg.sents(), OUTPUT_FILE)\n",
    "\n",
    "if success:\n",
    "    count = utils.num_found_idioms(OUTPUT_FILE)\n",
    "    print(\"number of idioms found (ish): {}\".format(count))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "008d5e52-9d63-49a8-a815-8e3ad5cb9a9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting 2script execution: 2022-12-30 19:02:48.118948\n",
      "2022-12-30 19:02:52.163869: Tagged 0 of 54716 sentences...\n",
      "2022-12-30 19:02:53.000825: Tagged 500 of 54716 sentences...\n",
      "2022-12-30 19:02:53.851599: Tagged 1000 of 54716 sentences...\n",
      "2022-12-30 19:02:54.741860: Tagged 1500 of 54716 sentences...\n",
      "2022-12-30 19:02:55.565755: Tagged 2000 of 54716 sentences...\n",
      "2022-12-30 19:02:56.380138: Tagged 2500 of 54716 sentences...\n",
      "2022-12-30 19:02:57.260858: Tagged 3000 of 54716 sentences...\n",
      "2022-12-30 19:02:58.177789: Tagged 3500 of 54716 sentences...\n",
      "2022-12-30 19:02:59.039552: Tagged 4000 of 54716 sentences...\n",
      "2022-12-30 19:02:59.911006: Tagged 4500 of 54716 sentences...\n",
      "2022-12-30 19:03:00.792247: Tagged 5000 of 54716 sentences...\n",
      "2022-12-30 19:03:01.659529: Tagged 5500 of 54716 sentences...\n",
      "2022-12-30 19:03:02.468780: Tagged 6000 of 54716 sentences...\n",
      "2022-12-30 19:03:03.316888: Tagged 6500 of 54716 sentences...\n",
      "2022-12-30 19:03:04.208106: Tagged 7000 of 54716 sentences...\n",
      "2022-12-30 19:03:05.039162: Tagged 7500 of 54716 sentences...\n",
      "2022-12-30 19:03:05.908393: Tagged 8000 of 54716 sentences...\n",
      "2022-12-30 19:03:06.790274: Tagged 8500 of 54716 sentences...\n",
      "2022-12-30 19:03:07.629632: Tagged 9000 of 54716 sentences...\n",
      "2022-12-30 19:03:08.428181: Tagged 9500 of 54716 sentences...\n",
      "2022-12-30 19:03:09.290800: Tagged 10000 of 54716 sentences...\n",
      "2022-12-30 19:03:10.200610: Tagged 10500 of 54716 sentences...\n",
      "2022-12-30 19:03:11.070065: Tagged 11000 of 54716 sentences...\n",
      "2022-12-30 19:03:11.930814: Tagged 11500 of 54716 sentences...\n",
      "2022-12-30 19:03:12.817838: Tagged 12000 of 54716 sentences...\n",
      "2022-12-30 19:03:13.720983: Tagged 12500 of 54716 sentences...\n",
      "2022-12-30 19:03:14.535652: Tagged 13000 of 54716 sentences...\n",
      "2022-12-30 19:03:15.419907: Tagged 13500 of 54716 sentences...\n",
      "2022-12-30 19:03:16.354303: Tagged 14000 of 54716 sentences...\n",
      "2022-12-30 19:03:17.188586: Tagged 14500 of 54716 sentences...\n",
      "2022-12-30 19:03:18.053807: Tagged 15000 of 54716 sentences...\n",
      "2022-12-30 19:03:18.921282: Tagged 15500 of 54716 sentences...\n",
      "2022-12-30 19:03:19.779093: Tagged 16000 of 54716 sentences...\n",
      "2022-12-30 19:03:20.571922: Tagged 16500 of 54716 sentences...\n",
      "2022-12-30 19:03:21.418075: Tagged 17000 of 54716 sentences...\n",
      "2022-12-30 19:03:22.299639: Tagged 17500 of 54716 sentences...\n",
      "2022-12-30 19:03:23.194189: Tagged 18000 of 54716 sentences...\n",
      "2022-12-30 19:03:24.052659: Tagged 18500 of 54716 sentences...\n",
      "2022-12-30 19:03:24.902346: Tagged 19000 of 54716 sentences...\n",
      "2022-12-30 19:03:25.785289: Tagged 19500 of 54716 sentences...\n",
      "2022-12-30 19:03:26.565790: Tagged 20000 of 54716 sentences...\n",
      "2022-12-30 19:03:27.416805: Tagged 20500 of 54716 sentences...\n",
      "2022-12-30 19:03:28.276310: Tagged 21000 of 54716 sentences...\n",
      "2022-12-30 19:03:29.142711: Tagged 21500 of 54716 sentences...\n",
      "2022-12-30 19:03:30.036167: Tagged 22000 of 54716 sentences...\n",
      "2022-12-30 19:03:30.922003: Tagged 22500 of 54716 sentences...\n",
      "2022-12-30 19:03:31.789437: Tagged 23000 of 54716 sentences...\n",
      "2022-12-30 19:03:32.570376: Tagged 23500 of 54716 sentences...\n",
      "2022-12-30 19:03:33.421752: Tagged 24000 of 54716 sentences...\n",
      "2022-12-30 19:03:34.322688: Tagged 24500 of 54716 sentences...\n",
      "2022-12-30 19:03:35.243379: Tagged 25000 of 54716 sentences...\n",
      "2022-12-30 19:03:36.115944: Tagged 25500 of 54716 sentences...\n",
      "2022-12-30 19:03:37.023192: Tagged 26000 of 54716 sentences...\n",
      "2022-12-30 19:03:37.911722: Tagged 26500 of 54716 sentences...\n",
      "2022-12-30 19:03:38.758929: Tagged 27000 of 54716 sentences...\n",
      "2022-12-30 19:03:39.584433: Tagged 27500 of 54716 sentences...\n",
      "2022-12-30 19:03:40.464231: Tagged 28000 of 54716 sentences...\n",
      "2022-12-30 19:03:41.346376: Tagged 28500 of 54716 sentences...\n",
      "2022-12-30 19:03:42.227645: Tagged 29000 of 54716 sentences...\n",
      "2022-12-30 19:03:43.122621: Tagged 29500 of 54716 sentences...\n",
      "2022-12-30 19:03:43.991136: Tagged 30000 of 54716 sentences...\n",
      "2022-12-30 19:03:44.983335: Tagged 30500 of 54716 sentences...\n",
      "2022-12-30 19:03:45.881271: Tagged 31000 of 54716 sentences...\n",
      "2022-12-30 19:03:46.667436: Tagged 31500 of 54716 sentences...\n",
      "2022-12-30 19:03:47.524742: Tagged 32000 of 54716 sentences...\n",
      "2022-12-30 19:03:48.374193: Tagged 32500 of 54716 sentences...\n",
      "2022-12-30 19:03:49.247175: Tagged 33000 of 54716 sentences...\n",
      "2022-12-30 19:03:50.121166: Tagged 33500 of 54716 sentences...\n",
      "2022-12-30 19:03:50.987693: Tagged 34000 of 54716 sentences...\n",
      "2022-12-30 19:03:51.865812: Tagged 34500 of 54716 sentences...\n",
      "2022-12-30 19:03:52.698145: Tagged 35000 of 54716 sentences...\n",
      "2022-12-30 19:03:53.568887: Tagged 35500 of 54716 sentences...\n",
      "2022-12-30 19:03:54.447581: Tagged 36000 of 54716 sentences...\n",
      "2022-12-30 19:03:55.320673: Tagged 36500 of 54716 sentences...\n",
      "2022-12-30 19:03:56.181514: Tagged 37000 of 54716 sentences...\n",
      "2022-12-30 19:03:57.028400: Tagged 37500 of 54716 sentences...\n",
      "2022-12-30 19:03:57.847228: Tagged 38000 of 54716 sentences...\n",
      "2022-12-30 19:03:58.675954: Tagged 38500 of 54716 sentences...\n",
      "2022-12-30 19:03:59.520763: Tagged 39000 of 54716 sentences...\n",
      "2022-12-30 19:04:00.393118: Tagged 39500 of 54716 sentences...\n",
      "2022-12-30 19:04:01.282186: Tagged 40000 of 54716 sentences...\n",
      "2022-12-30 19:04:02.146506: Tagged 40500 of 54716 sentences...\n",
      "2022-12-30 19:04:02.990431: Tagged 41000 of 54716 sentences...\n",
      "2022-12-30 19:04:03.790063: Tagged 41500 of 54716 sentences...\n",
      "2022-12-30 19:04:04.689616: Tagged 42000 of 54716 sentences...\n",
      "2022-12-30 19:04:05.536085: Tagged 42500 of 54716 sentences...\n",
      "2022-12-30 19:04:06.390254: Tagged 43000 of 54716 sentences...\n",
      "2022-12-30 19:04:07.270685: Tagged 43500 of 54716 sentences...\n",
      "2022-12-30 19:04:08.152971: Tagged 44000 of 54716 sentences...\n",
      "2022-12-30 19:04:09.012422: Tagged 44500 of 54716 sentences...\n",
      "2022-12-30 19:04:09.831026: Tagged 45000 of 54716 sentences...\n",
      "2022-12-30 19:04:10.772452: Tagged 45500 of 54716 sentences...\n",
      "2022-12-30 19:04:11.564131: Tagged 46000 of 54716 sentences...\n",
      "2022-12-30 19:04:12.418462: Tagged 46500 of 54716 sentences...\n",
      "2022-12-30 19:04:13.374646: Tagged 47000 of 54716 sentences...\n",
      "2022-12-30 19:04:14.171135: Tagged 47500 of 54716 sentences...\n",
      "2022-12-30 19:04:15.065840: Tagged 48000 of 54716 sentences...\n",
      "2022-12-30 19:04:15.897666: Tagged 48500 of 54716 sentences...\n",
      "2022-12-30 19:04:16.697551: Tagged 49000 of 54716 sentences...\n",
      "2022-12-30 19:04:17.627246: Tagged 49500 of 54716 sentences...\n",
      "2022-12-30 19:04:18.433921: Tagged 50000 of 54716 sentences...\n",
      "2022-12-30 19:04:19.356950: Tagged 50500 of 54716 sentences...\n",
      "2022-12-30 19:04:20.200769: Tagged 51000 of 54716 sentences...\n",
      "2022-12-30 19:04:21.096958: Tagged 51500 of 54716 sentences...\n",
      "2022-12-30 19:04:21.876947: Tagged 52000 of 54716 sentences...\n",
      "2022-12-30 19:04:22.691087: Tagged 52500 of 54716 sentences...\n",
      "2022-12-30 19:04:23.560781: Tagged 53000 of 54716 sentences...\n",
      "2022-12-30 19:04:24.371476: Tagged 53500 of 54716 sentences...\n",
      "2022-12-30 19:04:25.269159: Tagged 54000 of 54716 sentences...\n",
      "2022-12-30 19:04:26.120384: Tagged 54500 of 54716 sentences...\n",
      "Finishing script execution: 2022-12-30 19:04:26.466434\n",
      "number of idioms found (ish): 216\n"
     ]
    }
   ],
   "source": [
    "# reuters\n",
    "OUTPUT_FILE = \"./data/tagged_sentences_reuters\"\n",
    "\n",
    "sents = reuters.sents()\n",
    "\n",
    "# Clear contents if the file exists\n",
    "utils.clear_file(OUTPUT_FILE)\n",
    "\n",
    "\n",
    "## write to the file\n",
    "success = utils.write_tagged_sentences(reuters.sents(), OUTPUT_FILE)\n",
    "\n",
    "if success:\n",
    "    count = utils.num_found_idioms(OUTPUT_FILE)\n",
    "    print(\"number of idioms found (ish): {}\".format(count))\n",
    "\n",
    "\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493e6c03-0ea8-4b08-ab10-1474f76143f0",
   "metadata": {},
   "source": [
    "## Preparing to Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2f8cc0b-e944-428f-9fdf-ded2af3f414b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-30 19:27:37.005953: Gathering all the words...\n",
      "2022-12-30 19:27:37.400269: Lowercasing all the words...\n",
      "2022-12-30 19:28:05.393850: Lemmatizing all the words...\n",
      "2022-12-30 19:31:58.945690: Creating bigrams frequencies and storing results...\n",
      "2022-12-30 19:34:22.117761: Creating unigrams frequencies and storing results...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "OUT_PATH = \"./data/{}.pkl\"\n",
    "\n",
    "print(\"{}: Gathering all the words...\".format(datetime.now()))\n",
    "\n",
    "idiom_examples = pd.read_csv(\"./data/idiom_example.csv\")[\"sentence\"]\n",
    "\n",
    "idiom_examples_split = [nltk.word_tokenize(sent) for sent in idiom_examples]\n",
    "\n",
    "examples_words = [word for sent in idiom_examples_split \\\n",
    "                  for word in sent]\n",
    "\n",
    "# this doesn't work, a little salty about it\n",
    "#examples_words = [word for word_tokenize(sent) in idiom_examples \\\n",
    "#                  for word in word_tokenize(sent)]\n",
    "\n",
    "# Does it matter that the last sentence of one document will be combined with \n",
    "# the first sentence of another?\n",
    "words = brown.words() + gutenberg.words() + reuters.words() + examples_words\n",
    "\n",
    "print(\"{}: Lowercasing all the words...\".format(datetime.now()))\n",
    "words_lower = [w.lower() for w in words]\n",
    "\n",
    "print(\"{}: Lemmatizing all the words...\".format(datetime.now()))\n",
    "wnlt = nltk.WordNetLemmatizer()\n",
    "words_lemmatized = [wnlt.lemmatize(word, utils.get_wordnet_pos(tb_pos)) \\\n",
    "         for word,tb_pos in nltk.pos_tag(words_lower)]\n",
    "\n",
    "#bigrams = nltk.collocations.BigramCollocationFinder.from_words(\n",
    "#        words,\n",
    "#        window_size=20)\n",
    "\n",
    "#bigrams.apply_freq_filter(20)\n",
    "#bigrams_freq = bigrams.ngram_fd\n",
    "\n",
    "\n",
    "### TODO(?) : Try different windowsizes\n",
    "print(\"{}: Creating bigrams frequencies and storing results...\".format(datetime.now()))\n",
    "bigrams_freq = BigramCollocationFinder.from_words(words_lemmatized, window_size=20).ngram_fd\n",
    "\n",
    "with open(OUT_PATH.format(\"bigram_freq\"), \"wb\") as f:\n",
    "    pickle.dump(bigrams_freq, f)\n",
    "\n",
    "\n",
    "print(\"{}: Creating unigrams frequencies and storing results...\".format(datetime.now()))\n",
    "unigrams = nltk.FreqDist(words_lemmatized)\n",
    "unigrams_freq = nltk.FreqDist(words_lemmatized)\n",
    "#unigrams_freq = {unigram:freq for unigram, freq in unigrams.items() if freq >= 20}\n",
    "\n",
    "with open(OUT_PATH.format(\"unigrams_freq\"), \"wb\") as f:\n",
    "    pickle.dump(unigrams_freq, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97aeb9b-9b7b-4fd6-8aa9-0d2f68ce8de6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6000b8e-bb6d-45e2-8a15-ec293424abed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e44c50-4325-4929-b8f9-fbea62e8e376",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bcb495d-ac7e-457e-a637-cc41253f6004",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51ba266-b2e7-40b7-8000-3992a8bbffe2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b1a3e69f-3059-4d88-bad2-293193cb68c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-30 19:51:50.401799: Reading tagged sentences...\n",
      "2022-12-30 19:51:50.403072: Tagging sentences from ./data/tagged_sentences_brown...\n",
      "2022-12-30 19:52:37.571476: Finished!\n",
      "2022-12-30 19:52:37.571574: Adding newly tagged sentences to rest of sentences...\n",
      "2022-12-30 19:52:37.572799: Finished!\n",
      "2022-12-30 19:52:37.573082: Tagging sentences from ./data/tagged_sentences_reuters...\n",
      "2022-12-30 19:53:38.829862: Finished!\n",
      "2022-12-30 19:53:38.829939: Adding newly tagged sentences to rest of sentences...\n",
      "2022-12-30 19:53:38.831160: Finished!\n",
      "2022-12-30 19:53:38.831416: Tagging sentences from ./data/tagged_sentences_gutenberg...\n",
      "2022-12-30 19:55:22.582568: Finished!\n",
      "2022-12-30 19:55:22.582640: Adding newly tagged sentences to rest of sentences...\n",
      "2022-12-30 19:55:22.584902: Finished!\n",
      "2022-12-30 19:55:22.585158: Tagging sentences from ./data/tagged_sentences_examples...\n",
      "2022-12-30 19:55:22.758037: Finished!\n",
      "2022-12-30 19:55:22.758086: Adding newly tagged sentences to rest of sentences...\n",
      "2022-12-30 19:55:22.758108: Finished!\n",
      "2022-12-30 19:55:22.758317: Creating train and test datasets...\n",
      "2022-12-30 19:55:23.434223: Writing train data to file via pickle ...\n",
      "2022-12-30 19:55:23.512312: Writing test data to file via pickle ...\n",
      "2022-12-30 19:55:23.526584: Finished!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "INPUT = [\"./data/tagged_sentences_brown\",\n",
    "         \"./data/tagged_sentences_reuters\",\n",
    "         \"./data/tagged_sentences_gutenberg\",\n",
    "         \"./data/tagged_sentences_examples\"]\n",
    "\n",
    "TRAIN_PCT = 0.8\n",
    "UNDERSAMPLE_FACTOR = 4\n",
    "OUT_PATH = \"./data/{}.pkl\"\n",
    "SEED = 20190619\n",
    "\n",
    "\n",
    "print(\"{}: Reading tagged sentences...\".format(datetime.now()))\n",
    "tagged_sentences = []\n",
    "for fname in INPUT:\n",
    "    with open(fname, \"r\") as f: \n",
    "        print(\"{}: Tagging sentences from {}...\".format(datetime.now(), fname))\n",
    "        ts = [utils.tag_line(line) for line in f]\n",
    "        print(\"{}: Finished!\".format(datetime.now()))\n",
    "       \n",
    "        print(\"{}: Adding newly tagged sentences to rest of sentences...\"\n",
    "                      .format(datetime.now()))\n",
    "        tagged_sentences.extend(ts)\n",
    "        print(\"{}: Finished!\".format(datetime.now()))\n",
    "        \n",
    "\n",
    "print(\"{}: Creating train and test datasets...\".format(datetime.now()))\n",
    "\n",
    "train, test = utils.stratified_train_test(tagged_sentences,\n",
    "                                    SEED,\n",
    "                                    TRAIN_PCT,\n",
    "                                    UNDERSAMPLE_FACTOR)\n",
    "\n",
    "print(\"{}: Writing train data to file via pickle ...\".format(datetime.now()))\n",
    "\n",
    "with open(OUT_PATH.format(\"train\"), \"wb\") as f:\n",
    "    pickle.dump(train, f)\n",
    "\n",
    "print(\"{}: Writing test data to file via pickle ...\".format(datetime.now()))\n",
    "\n",
    "with open(OUT_PATH.format(\"test\"), \"wb\") as f:\n",
    "    pickle.dump(test, f)\n",
    "\n",
    "print(\"{}: Finished!\".format(datetime.now()))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79433f27-c1a3-4dcd-b06c-2ff5c277c2cf",
   "metadata": {},
   "source": [
    "## Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d0de33b7-8d2a-4028-be83-49459874a4eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Anybody', 'NN', 'OUT'),\n",
       " ('who', 'WP', 'OUT'),\n",
       " ('is', 'VBZ', 'OUT'),\n",
       " ('expecting', 'VBG', 'OUT'),\n",
       " ('a', 'DT', 'OUT'),\n",
       " ('joyride', 'NN', 'OUT'),\n",
       " ('should', 'MD', 'OUT'),\n",
       " (',', ',', 'OUT'),\n",
       " ('according', 'VBG', 'OUT'),\n",
       " ('to', 'TO', 'OUT'),\n",
       " ('Mr.', 'NNP', 'OUT'),\n",
       " ('Shriver', 'NNP', 'OUT'),\n",
       " (',', ',', 'OUT'),\n",
       " ('get', 'VB', 'OUT'),\n",
       " ('off', 'RP', 'OUT'),\n",
       " ('the', 'DT', 'OUT'),\n",
       " ('train', 'NN', 'OUT'),\n",
       " ('right', 'RB', 'OUT'),\n",
       " ('now', 'RB', 'OUT'),\n",
       " ('.', '.', 'OUT')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1967fb86-cb13-4f74-9261-c3a52520ac36",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pickle' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_30309/2123805344.py\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_PATH\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_PATH\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"test\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pickle' is not defined"
     ]
    }
   ],
   "source": [
    "DATA_PATH = \"./data/{}.pkl\"\n",
    "\n",
    "with open(DATA_PATH.format(\"train\"), \"rb\") as f:\n",
    "    train = pickle.load(f)\n",
    "    \n",
    "with open(DATA_PATH.format(\"test\"), \"rb\") as f:\n",
    "    test = pickle.load(f)\n",
    "\n",
    "train[0][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49d0c0a-964d-471c-ab0a-26f522d31aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(\"/media/yasser/ADERGHAL_M3_2TB/Software/GoogleNews-vectors-negative300.bin\"):\n",
    "    WORD2VEC = word2vec.KeyedVectors.load_word2vec_format(\n",
    "        \"/media/yasser/ADERGHAL_M3_2TB/Software/GoogleNews-vectors-negative300.bin\",\n",
    "        binary=True)\n",
    "else:\n",
    "    print(\"Pretrain Word2Vec model not found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053f7c2d-6cf4-4ef7-a529-5b4ddbbd59f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = utils.create_features(train, test, word2vec=WORD2VEC)\n",
    "\n",
    "X_train[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a10736-d612-4444-90df-3a66ea78bb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "crf = CRF(\n",
    "    algorithm='lbfgs',\n",
    "    c1=0.1,\n",
    "    c2=0.1,\n",
    "    max_iterations=100,\n",
    "    all_possible_transitions=False,\n",
    ")\n",
    "\n",
    "crf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "predictions = crf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebba1bc6-9c6e-4916-a133-855334ab1140",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(utils.explain_weights(crf, html=True))\n",
    "utils.print_classification_report(predictions, y_test)\n",
    "utils.binarized_confusion_matrix(predictions, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
